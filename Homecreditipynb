{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtNF1VM3d1Xhni6WI+YWH7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ysagar-hub/Ysagar-hub/blob/main/Homecreditipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOynV6qlHAD5",
        "outputId": "e0710b31-02ea-426d-b6c0-735aa285ac8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Step 1: Mounting Drive and Loading Data...\n",
            "Mounted at /content/drive/\n",
            "Train shape: (307511, 122), Test shape: (48744, 121)\n",
            "\n",
            ">>> Step 2A: Processing Bureau Data...\n",
            ">>> Step 2B: Processing Previous Applications...\n",
            ">>> Step 2C: Processing POS & Installments...\n",
            ">>> Step 2D: Processing Credit Card Balance...\n",
            ">>> Step 2E: Creating Domain Features & Fixes...\n",
            "\n",
            ">>> Step 3: Preprocessing...\n",
            "\n",
            ">>> Step 4: Starting K-Fold Training (This will take a few minutes)...\n",
            "  -> Training Fold 1...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2000]\tvalid_0's auc: 0.765251\tvalid_0's binary_logloss: 0.243869\n",
            "  -> Training Fold 2...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1798]\tvalid_0's auc: 0.773573\tvalid_0's binary_logloss: 0.241142\n",
            "  -> Training Fold 3...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1795]\tvalid_0's auc: 0.765826\tvalid_0's binary_logloss: 0.244077\n",
            "  -> Training Fold 4...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2000]\tvalid_0's auc: 0.773682\tvalid_0's binary_logloss: 0.241214\n",
            "  -> Training Fold 5...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1991]\tvalid_0's auc: 0.763769\tvalid_0's binary_logloss: 0.244379\n",
            "\n",
            "========================================\n",
            "FINAL CV SCORE: 0.76839\n",
            "========================================\n",
            "\n",
            ">>> Step 5: Saving Submission...\n",
            "Success! File 'submission_sagar.csv' is ready.\n",
            "   SK_ID_CURR    TARGET\n",
            "0      100001  0.027173\n",
            "1      100005  0.174402\n",
            "2      100013  0.021252\n",
            "3      100028  0.043997\n",
            "4      100038  0.178653\n"
          ]
        }
      ],
      "source": [
        "# COMPLETE END-TO-END HOME CREDIT PIPELINE (MASTER SCRIPT)\n",
        "# ======================================================================================\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from lightgbm import LGBMClassifier\n",
        "import lightgbm as lgb\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 1. SETUP & DATA LOADING\n",
        "print(\">>> Step 1: Mounting Drive and Loading Data...\")\n",
        "drive.mount('/content/drive/')\n",
        "data_dir = '/content/drive/MyDrive/mlp'\n",
        "if not os.path.exists(data_dir): os.makedirs(data_dir)\n",
        "os.chdir(data_dir)\n",
        "\n",
        "# Extract if needed\n",
        "if not os.path.exists('application_train.csv'):\n",
        "    zip_path = os.path.join(data_dir, 'home-credit-default-risk.zip')\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "\n",
        "# Load Main Data\n",
        "train = pd.read_csv('application_train.csv')\n",
        "test = pd.read_csv('application_test.csv')\n",
        "print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n",
        "\n",
        "# ======================================================================================\n",
        "# 2. FEATURE ENGINEERING (ALL FILES)\n",
        "# ======================================================================================\n",
        "\n",
        "# --- A. BUREAU DATA (Advanced Active/Closed Logic) ---\n",
        "print(\"\\n>>> Step 2A: Processing Bureau Data...\")\n",
        "if os.path.exists('bureau.csv'):\n",
        "    bureau = pd.read_csv('bureau.csv')\n",
        "\n",
        "    # 1. Basic Stats\n",
        "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({\n",
        "        'DAYS_CREDIT': 'mean',\n",
        "        'SK_ID_BUREAU': 'count'\n",
        "    }).reset_index()\n",
        "    bureau_agg.columns = ['SK_ID_CURR', 'BUREAU_DAYS_CREDIT_MEAN', 'BUREAU_LOAN_COUNT']\n",
        "\n",
        "    # 2. Active Loans\n",
        "    active = bureau[bureau['CREDIT_ACTIVE'] == 'Active']\n",
        "    active_agg = active.groupby('SK_ID_CURR').agg({'SK_ID_BUREAU': 'count'}).reset_index()\n",
        "    active_agg.columns = ['SK_ID_CURR', 'ACTIVE_LOANS_COUNT']\n",
        "\n",
        "    # 3. Closed Loans\n",
        "    closed = bureau[bureau['CREDIT_ACTIVE'] == 'Closed']\n",
        "    closed_agg = closed.groupby('SK_ID_CURR').agg({'SK_ID_BUREAU': 'count'}).reset_index()\n",
        "    closed_agg.columns = ['SK_ID_CURR', 'CLOSED_LOANS_COUNT']\n",
        "\n",
        "    # Merge\n",
        "    train = train.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
        "    train = train.merge(active_agg, on='SK_ID_CURR', how='left')\n",
        "    train = train.merge(closed_agg, on='SK_ID_CURR', how='left')\n",
        "\n",
        "    test = test.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
        "    test = test.merge(active_agg, on='SK_ID_CURR', how='left')\n",
        "    test = test.merge(closed_agg, on='SK_ID_CURR', how='left')\n",
        "\n",
        "    # Fill NaNs\n",
        "    for col in ['BUREAU_LOAN_COUNT', 'ACTIVE_LOANS_COUNT', 'CLOSED_LOANS_COUNT', 'BUREAU_DAYS_CREDIT_MEAN']:\n",
        "        train[col] = train[col].fillna(0)\n",
        "        test[col] = test[col].fillna(0)\n",
        "\n",
        "# --- B. PREVIOUS APPLICATIONS ---\n",
        "print(\">>> Step 2B: Processing Previous Applications...\")\n",
        "if os.path.exists('previous_application.csv'):\n",
        "    prev = pd.read_csv('previous_application.csv')\n",
        "    prev_agg = prev.groupby('SK_ID_CURR').agg({\n",
        "        'AMT_APPLICATION': 'mean',\n",
        "        'SK_ID_PREV': 'count',\n",
        "        'NAME_CONTRACT_STATUS': lambda x: (x == 'Approved').sum()\n",
        "    }).reset_index()\n",
        "    prev_agg.columns = ['SK_ID_CURR', 'PREV_AMT_APPLICATION_MEAN', 'PREV_TOTAL_APPS', 'PREV_APPROVED_COUNT']\n",
        "\n",
        "    train = train.merge(prev_agg, on='SK_ID_CURR', how='left')\n",
        "    test = test.merge(prev_agg, on='SK_ID_CURR', how='left')\n",
        "\n",
        "    for col in ['PREV_AMT_APPLICATION_MEAN', 'PREV_TOTAL_APPS', 'PREV_APPROVED_COUNT']:\n",
        "        train[col] = train[col].fillna(0)\n",
        "        test[col] = test[col].fillna(0)\n",
        "\n",
        "# --- C. POS CASH & INSTALLMENTS ---\n",
        "print(\">>> Step 2C: Processing POS & Installments...\")\n",
        "if os.path.exists('POS_CASH_balance.csv'):\n",
        "    pos = pd.read_csv('POS_CASH_balance.csv')\n",
        "    pos_agg = pos.groupby('SK_ID_CURR').agg({'SK_ID_PREV': 'count'}).reset_index()\n",
        "    pos_agg.columns = ['SK_ID_CURR', 'POS_COUNT']\n",
        "    train = train.merge(pos_agg, on='SK_ID_CURR', how='left')\n",
        "    test = test.merge(pos_agg, on='SK_ID_CURR', how='left')\n",
        "    train['POS_COUNT'] = train['POS_COUNT'].fillna(0)\n",
        "    test['POS_COUNT'] = test['POS_COUNT'].fillna(0)\n",
        "\n",
        "if os.path.exists('installments_payments.csv'):\n",
        "    install = pd.read_csv('installments_payments.csv')\n",
        "    install_agg = install.groupby('SK_ID_CURR').agg({'AMT_PAYMENT': 'sum'}).reset_index()\n",
        "    install_agg.columns = ['SK_ID_CURR', 'INSTAL_AMT_PAYMENT_SUM']\n",
        "    train = train.merge(install_agg, on='SK_ID_CURR', how='left')\n",
        "    test = test.merge(install_agg, on='SK_ID_CURR', how='left')\n",
        "    train['INSTAL_AMT_PAYMENT_SUM'] = train['INSTAL_AMT_PAYMENT_SUM'].fillna(0)\n",
        "    test['INSTAL_AMT_PAYMENT_SUM'] = test['INSTAL_AMT_PAYMENT_SUM'].fillna(0)\n",
        "\n",
        "# --- D. CREDIT CARD BALANCE ---\n",
        "print(\">>> Step 2D: Processing Credit Card Balance...\")\n",
        "if os.path.exists('credit_card_balance.csv'):\n",
        "    cc = pd.read_csv('credit_card_balance.csv')\n",
        "    cc_agg = cc.groupby('SK_ID_CURR').agg({'AMT_DRAWINGS_ATM_CURRENT': 'sum'}).reset_index()\n",
        "    cc_agg.columns = ['SK_ID_CURR', 'CC_AMT_DRAWINGS_SUM']\n",
        "    train = train.merge(cc_agg, on='SK_ID_CURR', how='left')\n",
        "    test = test.merge(cc_agg, on='SK_ID_CURR', how='left')\n",
        "    train['CC_AMT_DRAWINGS_SUM'] = train['CC_AMT_DRAWINGS_SUM'].fillna(0)\n",
        "    test['CC_AMT_DRAWINGS_SUM'] = test['CC_AMT_DRAWINGS_SUM'].fillna(0)\n",
        "\n",
        "# --- E. DOMAIN FEATURES & FIXES ---\n",
        "print(\">>> Step 2E: Creating Domain Features & Fixes...\")\n",
        "for df in [train, test]:\n",
        "    # Fix DAYS_EMPLOYED anomaly\n",
        "    df['DAYS_EMPLOYED_ANOM'] = df[\"DAYS_EMPLOYED\"] == 365243\n",
        "    df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].replace(365243, np.nan)\n",
        "\n",
        "    # Ratios\n",
        "    df['CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
        "    df['ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
        "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
        "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
        "\n",
        "    # External Sources Interaction\n",
        "    df['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
        "    df['EXT_SOURCES_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
        "\n",
        "# ======================================================================================\n",
        "# 3. PREPROCESSING\n",
        "# ======================================================================================\n",
        "print(\"\\n>>> Step 3: Preprocessing...\")\n",
        "y = train['TARGET']\n",
        "X = train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
        "X_test = test.drop(['SK_ID_CURR'], axis=1)\n",
        "\n",
        "# Encode Categoricals\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    # Convert to string to handle NaNs in categories\n",
        "    combined = pd.concat([X[col].astype(str), X_test[col].astype(str)])\n",
        "    le.fit(combined)\n",
        "    X[col] = le.transform(X[col].astype(str))\n",
        "    X_test[col] = le.transform(X_test[col].astype(str))\n",
        "\n",
        "# Fill remaining NaNs with mean\n",
        "train_mean = X.mean()\n",
        "X = X.fillna(train_mean).astype(np.float32)\n",
        "X_test = X_test.fillna(train_mean).astype(np.float32)\n",
        "\n",
        "# ======================================================================================\n",
        "# 4. K-FOLD MODEL TRAINING\n",
        "# ======================================================================================\n",
        "print(\"\\n>>> Step 4: Starting K-Fold Training (This will take a few minutes)...\")\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "test_preds = np.zeros(X_test.shape[0])\n",
        "oom_preds = np.zeros(X.shape[0])\n",
        "\n",
        "lgbm_params = {\n",
        "    'n_estimators': 2000,\n",
        "    'learning_rate': 0.01,\n",
        "    'num_leaves': 34,\n",
        "    'colsample_bytree': 0.85,\n",
        "    'subsample': 0.85,\n",
        "    'max_depth': 8,\n",
        "    'reg_alpha': 0.1,\n",
        "    'reg_lambda': 0.1,\n",
        "    'min_child_weight': 40,\n",
        "    'random_state': 42,\n",
        "    'verbosity': -1\n",
        "}\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
        "    print(f\"  -> Training Fold {fold_ + 1}...\")\n",
        "    X_trn, y_trn = X.iloc[trn_idx], y.iloc[trn_idx]\n",
        "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "\n",
        "    clf = LGBMClassifier(**lgbm_params)\n",
        "    clf.fit(\n",
        "        X_trn, y_trn,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric='auc',\n",
        "        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
        "    )\n",
        "\n",
        "    oom_preds[val_idx] = clf.predict_proba(X_val)[:, 1]\n",
        "    test_preds += clf.predict_proba(X_test)[:, 1] / folds.n_splits\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"FINAL CV SCORE: {roc_auc_score(y, oom_preds):.5f}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# ======================================================================================\n",
        "# 5. SUBMISSION\n",
        "# ======================================================================================\n",
        "print(\"\\n>>> Step 5: Saving Submission...\")\n",
        "submission = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': test_preds})\n",
        "\n",
        "# Saving with the name you requested\n",
        "submission.to_csv('submission_sagar.csv', index=False)\n",
        "print(\"Success! File 'submission_sagar.csv' is ready.\")\n",
        "print(submission.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================================\n",
        "# ULTIMATE HOME CREDIT ENSEMBLE PIPELINE (LGBM + XGBOOST)\n",
        "# ======================================================================================\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import re\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from lightgbm import LGBMClassifier\n",
        "import lightgbm as lgb\n",
        "from xgboost import XGBClassifier\n",
        "from google.colab import drive\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 1. SETUP & DATA LOADING\n",
        "print(\">>> Step 1: Mounting Drive and Loading Data...\")\n",
        "drive.mount('/content/drive/')\n",
        "data_dir = '/content/drive/MyDrive/mlp'\n",
        "if not os.path.exists(data_dir): os.makedirs(data_dir)\n",
        "os.chdir(data_dir)\n",
        "\n",
        "if not os.path.exists('application_train.csv'):\n",
        "    with zipfile.ZipFile('home-credit-default-risk.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "\n",
        "train = pd.read_csv('application_train.csv')\n",
        "test = pd.read_csv('application_test.csv')\n",
        "print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n",
        "\n",
        "# ======================================================================================\n",
        "# 2. FEATURE ENGINEERING (ADVANCED)\n",
        "# ======================================================================================\n",
        "print(\"\\n>>> Step 2: Feature Engineering...\")\n",
        "\n",
        "# --- A. BUREAU ---\n",
        "if os.path.exists('bureau.csv'):\n",
        "    bureau = pd.read_csv('bureau.csv')\n",
        "    # Basic\n",
        "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({'DAYS_CREDIT': 'mean', 'SK_ID_BUREAU': 'count'}).reset_index()\n",
        "    bureau_agg.columns = ['SK_ID_CURR', 'BUREAU_DAYS_CREDIT_MEAN', 'BUREAU_LOAN_COUNT']\n",
        "    # Active\n",
        "    active = bureau[bureau['CREDIT_ACTIVE'] == 'Active']\n",
        "    active_agg = active.groupby('SK_ID_CURR').agg({'SK_ID_BUREAU': 'count'}).reset_index()\n",
        "    active_agg.columns = ['SK_ID_CURR', 'ACTIVE_LOANS_COUNT']\n",
        "    # Closed\n",
        "    closed = bureau[bureau['CREDIT_ACTIVE'] == 'Closed']\n",
        "    closed_agg = closed.groupby('SK_ID_CURR').agg({'SK_ID_BUREAU': 'count'}).reset_index()\n",
        "    closed_agg.columns = ['SK_ID_CURR', 'CLOSED_LOANS_COUNT']\n",
        "\n",
        "    for df in [train, test]:\n",
        "        df = df.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
        "        df = df.merge(active_agg, on='SK_ID_CURR', how='left')\n",
        "        df = df.merge(closed_agg, on='SK_ID_CURR', how='left')\n",
        "        for col in ['BUREAU_LOAN_COUNT', 'ACTIVE_LOANS_COUNT', 'CLOSED_LOANS_COUNT']:\n",
        "            df[col] = df[col].fillna(0)\n",
        "\n",
        "# --- B. PREVIOUS APPS ---\n",
        "if os.path.exists('previous_application.csv'):\n",
        "    prev = pd.read_csv('previous_application.csv')\n",
        "    prev_agg = prev.groupby('SK_ID_CURR').agg({\n",
        "        'AMT_APPLICATION': 'mean', 'SK_ID_PREV': 'count',\n",
        "        'NAME_CONTRACT_STATUS': lambda x: (x == 'Approved').sum()\n",
        "    }).reset_index()\n",
        "    prev_agg.columns = ['SK_ID_CURR', 'PREV_AMT_APPLICATION_MEAN', 'PREV_TOTAL_APPS', 'PREV_APPROVED_COUNT']\n",
        "\n",
        "    train = train.merge(prev_agg, on='SK_ID_CURR', how='left')\n",
        "    test = test.merge(prev_agg, on='SK_ID_CURR', how='left')\n",
        "\n",
        "    for col in ['PREV_AMT_APPLICATION_MEAN', 'PREV_TOTAL_APPS', 'PREV_APPROVED_COUNT']:\n",
        "        train[col] = train[col].fillna(0)\n",
        "        test[col] = test[col].fillna(0)\n",
        "\n",
        "# --- C. POS & INSTALLMENTS & CC ---\n",
        "if os.path.exists('POS_CASH_balance.csv'):\n",
        "    pos = pd.read_csv('POS_CASH_balance.csv')\n",
        "    pos_agg = pos.groupby('SK_ID_CURR').agg({'SK_ID_PREV': 'count'}).reset_index()\n",
        "    pos_agg.columns = ['SK_ID_CURR', 'POS_COUNT']\n",
        "    train = train.merge(pos_agg, on='SK_ID_CURR', how='left')\n",
        "    test = test.merge(pos_agg, on='SK_ID_CURR', how='left')\n",
        "    train['POS_COUNT'] = train['POS_COUNT'].fillna(0); test['POS_COUNT'] = test['POS_COUNT'].fillna(0)\n",
        "\n",
        "if os.path.exists('installments_payments.csv'):\n",
        "    install = pd.read_csv('installments_payments.csv')\n",
        "    install_agg = install.groupby('SK_ID_CURR').agg({'AMT_PAYMENT': 'sum'}).reset_index()\n",
        "    install_agg.columns = ['SK_ID_CURR', 'INSTAL_AMT_PAYMENT_SUM']\n",
        "    train = train.merge(install_agg, on='SK_ID_CURR', how='left')\n",
        "    test = test.merge(install_agg, on='SK_ID_CURR', how='left')\n",
        "    train['INSTAL_AMT_PAYMENT_SUM'] = train['INSTAL_AMT_PAYMENT_SUM'].fillna(0); test['INSTAL_AMT_PAYMENT_SUM'] = test['INSTAL_AMT_PAYMENT_SUM'].fillna(0)\n",
        "\n",
        "if os.path.exists('credit_card_balance.csv'):\n",
        "    cc = pd.read_csv('credit_card_balance.csv')\n",
        "    cc_agg = cc.groupby('SK_ID_CURR').agg({'AMT_DRAWINGS_ATM_CURRENT': 'sum'}).reset_index()\n",
        "    cc_agg.columns = ['SK_ID_CURR', 'CC_AMT_DRAWINGS_SUM']\n",
        "    train = train.merge(cc_agg, on='SK_ID_CURR', how='left')\n",
        "    test = test.merge(cc_agg, on='SK_ID_CURR', how='left')\n",
        "    train['CC_AMT_DRAWINGS_SUM'] = train['CC_AMT_DRAWINGS_SUM'].fillna(0); test['CC_AMT_DRAWINGS_SUM'] = test['CC_AMT_DRAWINGS_SUM'].fillna(0)\n",
        "\n",
        "# --- D. DOMAIN FEATURES ---\n",
        "print(\">>> Step 2D: Domain Features...\")\n",
        "for df in [train, test]:\n",
        "    df['DAYS_EMPLOYED_ANOM'] = df[\"DAYS_EMPLOYED\"] == 365243\n",
        "    df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].replace(365243, np.nan)\n",
        "    df['CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
        "    df['ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
        "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
        "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
        "    df['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
        "    df['EXT_SOURCES_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
        "\n",
        "# ======================================================================================\n",
        "# 3. PREPROCESSING\n",
        "# ======================================================================================\n",
        "print(\"\\n>>> Step 3: Preprocessing...\")\n",
        "y = train['TARGET']\n",
        "X = train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
        "X_test = test.drop(['SK_ID_CURR'], axis=1)\n",
        "\n",
        "# Categorical Encoding\n",
        "for col in X.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    combined = pd.concat([X[col].astype(str), X_test[col].astype(str)])\n",
        "    le.fit(combined)\n",
        "    X[col] = le.transform(X[col].astype(str))\n",
        "    X_test[col] = le.transform(X_test[col].astype(str))\n",
        "\n",
        "# XGBoost Requirement: Rename columns to remove spaces or special chars\n",
        "X.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X.columns]\n",
        "X_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test.columns]\n",
        "\n",
        "# Fill NaNs\n",
        "train_mean = X.mean()\n",
        "X = X.fillna(train_mean).astype(np.float32)\n",
        "X_test = X_test.fillna(train_mean).astype(np.float32)\n",
        "\n",
        "# ======================================================================================\n",
        "# 4. ENSEMBLE TRAINING (LGBM + XGBOOST)\n",
        "# ======================================================================================\n",
        "print(\"\\n>>> Step 4: Starting Ensemble Training...\")\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# --- MODEL 1: LIGHTGBM ---\n",
        "print(\">>> Training LightGBM (5 Folds)...\")\n",
        "lgb_preds = np.zeros(X_test.shape[0])\n",
        "lgbm_params = {\n",
        "    'n_estimators': 2000, 'learning_rate': 0.01, 'num_leaves': 34, 'colsample_bytree': 0.85,\n",
        "    'subsample': 0.85, 'max_depth': 8, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'min_child_weight': 40,\n",
        "    'random_state': 42, 'verbosity': -1\n",
        "}\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
        "    X_trn, y_trn = X.iloc[trn_idx], y.iloc[trn_idx]\n",
        "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "    clf = LGBMClassifier(**lgbm_params)\n",
        "    clf.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], eval_metric='auc',\n",
        "            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)])\n",
        "    lgb_preds += clf.predict_proba(X_test)[:, 1] / 5\n",
        "\n",
        "# --- MODEL 2: XGBOOST ---\n",
        "print(\">>> Training XGBoost (5 Folds)...\")\n",
        "xgb_preds = np.zeros(X_test.shape[0])\n",
        "xgb_params = {\n",
        "    'n_estimators': 2000, 'learning_rate': 0.01, 'max_depth': 8, 'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8, 'tree_method': 'hist', 'random_state': 42, 'eval_metric': 'auc',\n",
        "    'early_stopping_rounds': 100\n",
        "}\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
        "    X_trn, y_trn = X.iloc[trn_idx], y.iloc[trn_idx]\n",
        "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "    clf = XGBClassifier(**xgb_params)\n",
        "    clf.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], verbose=0)\n",
        "    xgb_preds += clf.predict_proba(X_test)[:, 1] / 5\n",
        "\n",
        "# --- BLENDING ---\n",
        "print(\"\\n>>> Blending Predictions (50% LGBM + 50% XGBoost)...\")\n",
        "final_preds = (0.5 * lgb_preds) + (0.5 * xgb_preds)\n",
        "\n",
        "# ======================================================================================\n",
        "# 5. SUBMISSION\n",
        "# ======================================================================================\n",
        "submission = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': final_preds})\n",
        "submission.to_csv('submission_sagar_ensemble.csv', index=False)\n",
        "print(\"\\nSUCCESS! Final file 'submission_sagar_ensemble.csv' created.\")\n",
        "print(submission.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAdMUI2_IWwo",
        "outputId": "349625e0-75ae-4fe9-d412-dcb05e63bc15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Step 1: Mounting Drive and Loading Data...\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Train shape: (307511, 122), Test shape: (48744, 121)\n",
            "\n",
            ">>> Step 2: Feature Engineering...\n",
            ">>> Step 2D: Domain Features...\n",
            "\n",
            ">>> Step 3: Preprocessing...\n",
            "\n",
            ">>> Step 4: Starting Ensemble Training...\n",
            ">>> Training LightGBM (5 Folds)...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2000]\tvalid_0's auc: 0.763101\tvalid_0's binary_logloss: 0.244519\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1995]\tvalid_0's auc: 0.773305\tvalid_0's binary_logloss: 0.241305\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1966]\tvalid_0's auc: 0.763444\tvalid_0's binary_logloss: 0.244882\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2000]\tvalid_0's auc: 0.771411\tvalid_0's binary_logloss: 0.241941\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1999]\tvalid_0's auc: 0.761985\tvalid_0's binary_logloss: 0.244911\n",
            ">>> Training XGBoost (5 Folds)...\n",
            "\n",
            ">>> Blending Predictions (50% LGBM + 50% XGBoost)...\n",
            "\n",
            "SUCCESS! Final file 'submission_sagar_ensemble.csv' created.\n",
            "   SK_ID_CURR    TARGET\n",
            "0      100001  0.027131\n",
            "1      100005  0.151422\n",
            "2      100013  0.021414\n",
            "3      100028  0.039520\n",
            "4      100038  0.183819\n"
          ]
        }
      ]
    }
  ]
}